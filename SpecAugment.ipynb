{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpecAugment with TorchAudio\n",
    "by [Zach Caceres](https://github.com/zcaceres) and [Jenny Cai](https://github.com/qcai2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook feature a Pytorch implementation of the data augmentation techniques in [SpecAugment](https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html) by Google Brain.\n",
    "\n",
    "The techniques are:\n",
    "- Time Warp\n",
    "- Frequency Mask\n",
    "- Time Mask\n",
    "\n",
    "SpecAugment reports that Frequency Mask and Time Mask were the most effective augmentations and less computationally expensive than Time Warp. We included Time Warp for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "Be sure you've run `install.sh` before running this notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchaudio'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-39a1f4e5c821>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtorchaudio\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtorchaudio\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mtransforms\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torchaudio'"
     ]
    }
   ],
   "source": [
    "#Export\n",
    "from collections import namedtuple\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converts our implementation of `sparse_image_warp` into a Python file we can import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "python notebook2script.py SparseImageWarp.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_SparseImageWarp import sparse_image_warp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out our sample wav file (a snippet of background chatter from a party). We'll load it into a `namedtuple` to keep the signal & sample rate tied together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = './party-crowd.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AudioData = namedtuple('AudioData', ['sig', 'sr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioData(*torchaudio.load(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_audio(aud):\n",
    "    display(Audio(data=aud.sig, rate=aud.sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_audio(audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav to (Mel)Spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple wrapper for `torchaudio`'s transforms to make a decibel-scale Melspectrogram. Borrowed from [fastai-audio](https://github.com/zcaceres/fastai-audio), with some tweaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfm_spectro(ad:Audio, sr=16000, to_db_scale=False, n_fft=1024, \n",
    "                ws=None, hop=None, f_min=0.0, f_max=-80, pad=0, n_mels=128):\n",
    "    # We must reshape signal for torchaudio to generate the spectrogram.\n",
    "    mel = transforms.MelSpectrogram(sr=ad.sr, n_mels=n_mels, n_fft=n_fft, ws=ws, hop=hop, \n",
    "                                    f_min=f_min, f_max=f_max, pad=pad,)(ad.sig.reshape(1, -1))\n",
    "    mel = mel.permute(0,2,1) # swap dimension, mostly to look sane to a human.\n",
    "    if to_db_scale: mel = transforms.SpectrogramToDB(stype='magnitude', top_db=f_max)(mel)\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectro = tfm_spectro(audio, ws=512, hop=256, n_mels=128, to_db_scale=True, f_max=8000, f_min=-80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run this cell if you want to use your spectrogram to run the SparseImageWarp notebook.\n",
    "# %store spectro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing a spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_img(spectrogram):\n",
    "    plt.figure(figsize=(14,1)) # arbitrary, looks good on my screen.\n",
    "    plt.imshow(spectrogram[0])\n",
    "    plt.show()\n",
    "    display(spectrogram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_img(spectro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Warp\n",
    "\n",
    "Time Warp is described as:\n",
    "> Time warping is applied via the function sparse image warp of tensorflow. Given a log mel spectrogram with τ time steps, we view it as an image where the time axis is horizontal and the frequency axis is vertical. A random point along the horizontal line passing through the center of the image within the time steps (W, τ − W) is to be warped either to the left or right by a distance w chosen from a uniform distribution from 0 to the time warp parameter W along that line.\n",
    "\n",
    "Intuitively we squish/stretch the audio in a given direction bounded by the `W` parameter we select.\n",
    "\n",
    "NOTE:\n",
    "`sparse_image_warp` does not exist in Pytorch, so you can see our implementation in the SparseImageWarp.ipynb notebook. Our implementation skips certain features like add clamping.\n",
    "\n",
    "You should be careful with your `W` parameter because it's highly dependent on your audio and spectrogram. \n",
    "\n",
    "*Sanity check your spectrograms if you're using this transform.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "def time_warp(spec, W=5):\n",
    "    num_rows = spec.shape[1]\n",
    "    spec_len = spec.shape[2]\n",
    "    device = spec.device\n",
    "\n",
    "    # adapted from https://github.com/DemisEom/SpecAugment/\n",
    "    pt = (num_rows - 2* W) * torch.rand([], dtype=torch.int) + W # random point along the time axis\n",
    "    src_ctr_pt_freq = torch.range(0, spec_len // 2)  # control points on freq-axis\n",
    "    src_ctr_pt_time = torch.ones_like(src_ctr_pt_freq) * pt  # control points on time-axis\n",
    "    src_ctr_pts = torch.stack((src_ctr_pt_time, src_ctr_pt_freq), dim=-1)\n",
    "    src_ctr_pts = src_ctr_pts.float().to(device)\n",
    "\n",
    "    # Destination\n",
    "    w = 2 * W * torch.rand([], dtype=torch.int) - W# distance\n",
    "    dest_ctr_pt_freq = src_ctr_pt_freq\n",
    "    dest_ctr_pt_time = src_ctr_pt_time + w\n",
    "    dest_ctr_pts = torch.stack((dest_ctr_pt_time, dest_ctr_pt_freq), dim=-1)\n",
    "    dest_ctr_pts = dest_ctr_pts.float().to(device)\n",
    "\n",
    "    # warp\n",
    "    source_control_point_locations = torch.unsqueeze(src_ctr_pts, 0)  # (1, v//2, 2)\n",
    "    dest_control_point_locations = torch.unsqueeze(dest_ctr_pts, 0)  # (1, v//2, 2)\n",
    "\n",
    "    warped_spectro, dense_flows = sparse_image_warp(spec, source_control_point_locations, dest_control_point_locations)\n",
    "    return warped_spectro.squeeze(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_warp():\n",
    "    tensor_to_img(time_warp(spectro))\n",
    "tensor_to_img(spectro)    \n",
    "test_time_warp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Mask\n",
    "\n",
    "Frequency masking is described as:\n",
    "> Frequency masking is applied so that f consecutive mel\n",
    "frequency channels [f0, f0 + f) are masked, where f is first chosen from a uniform distribution from 0 to the frequency mask parameter F, and f0 is chosen from 0, ν − f). ν is the number of mel frequency channels.\n",
    "\n",
    "Intuitively, this is similar to cutout in computer vision workflows. We mask certain frequency bands with either the mean value of the spectrogram or zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "def freq_mask(spec, F=30, num_masks=1, replace_with_zero=False):\n",
    "    cloned = spec.clone()\n",
    "    num_mel_channels = cloned.shape[1]\n",
    "    \n",
    "    for i in range(0, num_masks):        \n",
    "        f = random.randrange(0, F)\n",
    "        f_zero = random.randrange(0, num_mel_channels - f)\n",
    "\n",
    "        # avoids randrange error if values are equal and range is empty\n",
    "        if (f_zero == f_zero + f): return cloned\n",
    "\n",
    "        mask_end = random.randrange(f_zero, f_zero + f) \n",
    "        if (replace_with_zero): cloned[0][f_zero:mask_end] = 0\n",
    "        else: cloned[0][f_zero:mask_end] = cloned.mean()\n",
    "    \n",
    "    return cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_freq_mask():\n",
    "    tensor_to_img(freq_mask(spectro))\n",
    "    # Two Masks...\n",
    "    tensor_to_img(freq_mask(spectro, num_masks=2))\n",
    "    # with zeros\n",
    "    tensor_to_img(freq_mask(spectro, num_masks=2, replace_with_zero=True))\n",
    "test_freq_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Mask\n",
    "\n",
    "Time masking is described as:\n",
    "> Time masking is applied so that t consecutive time steps\n",
    "[t0, t0 + t) are masked, where t is first chosen from a uniform distribution from 0 to the time mask parameter T, and t0 is chosen from [0, τ − t). We introduce an upper bound on the time mask so that a time mask cannot be wider than p times the number of time steps.\n",
    "\n",
    "Intuitively, this is similar to cutout and the frequency mask above. We mask certain time ranges with the mean value of the spectrogram or zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "def time_mask(spec, T=40, num_masks=1, replace_with_zero=False):\n",
    "    cloned = spec.clone()\n",
    "    len_spectro = cloned.shape[2]\n",
    "    \n",
    "    for i in range(0, num_masks):\n",
    "        t = random.randrange(0, T)\n",
    "        t_zero = random.randrange(0, len_spectro - t)\n",
    "\n",
    "        # avoids randrange error if values are equal and range is empty\n",
    "        if (t_zero == t_zero + t): return cloned\n",
    "\n",
    "        mask_end = random.randrange(t_zero, t_zero + t)\n",
    "        if (replace_with_zero): cloned[0][:,t_zero:mask_end] = 0\n",
    "        else: cloned[0][:,t_zero:mask_end] = cloned.mean()\n",
    "    return cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_time_mask():\n",
    "    tensor_to_img(time_mask(spectro))\n",
    "    # Two Masks...\n",
    "    tensor_to_img(time_mask(spectro, num_masks=2))\n",
    "    # with zeros\n",
    "    tensor_to_img(time_mask(spectro, num_masks=2, replace_with_zero=True))\n",
    "test_time_mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined\n",
    "Here we combine all three transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = time_mask(freq_mask(time_warp(spectro), num_masks=2), num_masks=2)\n",
    "tensor_to_img(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "time_warp(spectro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "freq_mask(spectro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "time_mask(spectro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 10\n",
    "time_mask(freq_mask(time_warp(spectro), num_masks=2), num_masks=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cspectro = spectro.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "time_warp(cspectro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "freq_mask(cspectro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "time_mask(cspectro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 100\n",
    "time_mask(freq_mask(time_warp(cspectro), num_masks=2), num_masks=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}